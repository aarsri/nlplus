---
layout: post
title: David Chiang
---

Lunch at 12:30pm, talk at 1pm, in 148 Fitzpatrick

Title:
Overcoming a Theoretical Limitation of Self-Attention

Abstract: Although transformers are remarkably effective for many tasks, there are some surprisingly easy-looking regular languages that they struggle with. Hahn shows that for languages where acceptance depends on a single input symbol, a transformer's classification decisions become less and less confident (that is, with cross-entropy approaching 1 bit per string) as input strings get longer and longer. We examine this limitation using two languages: PARITY, the language of bit strings with an odd number of 1s, and FIRST, the language of bit strings starting with a 1. We demonstrate three ways of overcoming the limitation suggested by Hahn's lemma. First, we settle an open question by constructing a transformer that recognizes PARITY with perfect accuracy, and similarly for FIRST. Second, we use layer normalization to bring the cross-entropy of both models arbitrarily close to zero. Third, when transformers need to focus on a single position, as for FIRST, we find that they can fail to generalize to longer strings; we offer a simple remedy to this problem that also improves length generalization in machine translation. 

Bio:Dr. Chiang's research is in natural language processing, the subfield of computer science that aims to enable computers to understand and produce human language. He focus's mainly on language translation, and am interested in syntactic parsing and other areas as well.


